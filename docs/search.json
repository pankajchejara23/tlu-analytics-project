[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/data.html",
    "href": "posts/data.html",
    "title": "Learning Trajectories Dataset",
    "section": "",
    "text": "This dataset captures student interactions with VARA, a digital learning platform where educational materials are organized as structured learning trajectories. These trajectories consist of:\n\nSequential Episodes\n\nArranged in dependent order\n\nEach targets a specific concept or skill\n\nFlexible Activities\n\nStudent-directed exploration within episodes\n\nMultiple attempts allowed\n\nBuilt-in hints and supplementary materials\n\n\nStudents interactions to learning trajectories are recorded and processed to extract behavioral features such as number of attempts, time spent, completions, etc. Additionally, students‚Äô metacognitive indicators are obtained through self-reported data.\n\nDataset samples\nThe dataset contains interaction records of 108 students with 14 episodes. The snapshots below provide samples of dataset.\n\n\nCode\nimport pandas as pd\n\n# load data\ndf = pd.read_csv('../Eduflex.xlsx - Pankajile.csv')\n\n# columns containing knowledge-related data\nknowledge_cols = ['pre_knowl','post_knowl','knowledge gain']\n\n# columns containing self-reported data\nself_report_cols = ['T5_learning difficulty',\n                     'T2_Post_effectiveness',\n                     'T4_Post_needforhelp',\n                     'T7_shared_control_student',\n                     'T8_shared_teacher',\n                     'T11__cogn_load',\n                     'T13_own_effort',\n                     'T14_TAM_easy']\n\n# columns containing episodes data\nepisodes_cols =[item for item in df.columns.to_list() if item not in knowledge_cols + self_report_cols + ['Gender']]\n\n# knowledge data\ndf_knowledge = df[knowledge_cols]\n\n# self-reported data\ndf_self_report = df[self_report_cols]\n\n# episodes data\ndf_episodes = df[episodes_cols]\n\ndf_knowledge.head()\n\n\n\n\n\n\n\n\n\npre_knowl\npost_knowl\nknowledge gain\n\n\n\n\n0\n0.49\n0.60\n0.11\n\n\n1\n0.86\n0.97\n0.11\n\n\n2\nNaN\nNaN\nNaN\n\n\n3\n0.93\n0.97\n0.04\n\n\n4\n0.88\n0.87\n-0.01\n\n\n\n\n\n\n\n\n\nCode\ndf.shape\n\n\n(108, 133)\n\n\n\n\nCode\ndf_self_report.head()\n\n\n\n\n\n\n\n\n\nT5_learning difficulty\nT2_Post_effectiveness\nT4_Post_needforhelp\nT7_shared_control_student\nT8_shared_teacher\nT11__cogn_load\nT13_own_effort\nT14_TAM_easy\n\n\n\n\n0\nNaN\n4.67\n4.5\n5.00\n3.00\n1.00\n5.00\n3.8\n\n\n1\n3.4\n1.67\n3.0\n5.00\n5.00\n5.00\n5.00\n4.2\n\n\n2\n3.6\n3.00\n3.0\n3.25\n3.00\n3.00\n3.00\n3.6\n\n\n3\n2.8\n3.00\n2.0\n3.25\n4.75\n2.63\n3.75\n3.8\n\n\n4\n2.6\n2.67\n3.0\n2.75\n3.75\n2.88\n2.75\n4.6\n\n\n\n\n\n\n\n\n\nCode\ndf_episodes.head()\n\n\n\n\n\n\n\n\n\nStudent_ID\nE1\nE1_total_tasks\nE1_tasks_completed\nE1_task_complexity\nE1_total_hints\nE1_hints_used\nE1_total_materials\nE1_addMat_used\nE1total_activity_count\n...\nE13_total_activity_count\nE14\nE14_total_tasks\nE14_tasks_completed\nE14_task_complexity\nE14_total hints\nE14_total_hints_used\nE14_total materials\nE14__addMat_used\nE14_total_activity\n\n\n\n\n0\nkeila1\n0.22\n8\n87.5\n1.43\n3\n0.00\n2\n0\n42\n...\n46\n284\n8\n62.5\n1.8\n0\n0\n4\n0\n82\n\n\n1\nkeila2\n0.79\n8\n87.5\n1.43\n3\n66.67\n2\n0\n75\n...\n46\n284\n8\n62.5\n1.8\n0\n0\n4\n0\n79\n\n\n2\nkeila3\n75.00\n8\n37.5\n1.33\n3\n0.00\n2\n0\n28\n...\n62\n284\n8\n12.5\n1.0\n0\n0\n4\n0\n39\n\n\n3\nkeila4\n75.00\n8\n50.0\n1.50\n3\n66.67\n2\n0\n73\n...\n55\n284\n8\n62.5\n1.8\n0\n0\n4\n0\n138\n\n\n4\nkeila5\n75.00\n8\n50.0\n1.50\n3\n33.33\n2\n0\n41\n...\n94\n284\n8\n62.5\n1.8\n0\n0\n4\n0\n66\n\n\n\n\n5 rows √ó 121 columns\n\n\n\n\n\n\n\n\n\nMissing values\n\n\n\nSome of the data columns in the datasets contain missing values which need to be handled before moving to analysis."
  },
  {
    "objectID": "posts/predict.html",
    "href": "posts/predict.html",
    "title": "Predicting students‚Äô perceived learning effectiveness and knowledge gain",
    "section": "",
    "text": "We will now investigate the predictive power of behavioral features along with self-reported measures to predict students perceived learning effectiveness and knowledge gain.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_predict, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Load episodes data\ndf_ep = pd.read_csv('uncorreled_features.csv')\ndf_ep.set_index('student_id', inplace=True)\n\n# Load knowledge gain data\ndf_kno = pd.read_csv('knowledge_clean.csv')\ndf_kno.set_index('student_id', inplace=True)\n\n# Load self reported data\ndf_sel = pd.read_csv('self_report_clean.csv')\ndf_sel.set_index('student_id', inplace=True)\n\nclean_df = pd.concat([df_ep,df_kno,df_sel], join='inner', axis=1)\n\nindp = ['adjusted_hints_used_mean', 'adjusted_tasks_completed_mean', 'adjusted_tasks_completed_q1',\n       'adjusted_activities_mean', 'adjusted_activities_std']\n\ntargets = ['knowledge gain','T2_Post_effectiveness']\ntarget_split_point = {'knowledge gain':clean_df['knowledge gain'].median(),'T2_Post_effectiveness':clean_df['T2_Post_effectiveness'].median()}\n\n\n\nPrediction using behavioral features\nWe will predict two key outcomes using behavioral features:Perceived Effectiveness, Knowledge Gain\nTo transform these outcomes into a binary variable for classification, we will perform steps given below. - Perform median split on each target variable: - Median split:\n- High ‚â• Median value\n- Low &lt; Median value\n\n\nCode\nresults = {}\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create a copy of the dataframe to avoid modifying the original\ndf_with_binary_targets = clean_df.copy()\n\nfor target in targets:\n    #print(f\"\\n{'='*70}\")\n    #print(f\"Building Random Forest classification model for target: {target}\")\n    \n    # Create binary target using median split\n    median_value = target_split_point[target]\n    binary_target_name = f\"{target}_binary\"\n    df_with_binary_targets[binary_target_name] = (clean_df[target] &gt; median_value).astype(int)\n    \n    #print(f\"Class distribution: {df_with_binary_targets[binary_target_name].value_counts().to_dict()}\")\n    \n    # Prepare X and y\n    X = df_with_binary_targets[indp]\n    y = df_with_binary_targets[binary_target_name]\n    \n    # Create Random Forest classifier with default parameters\n    rf_model = RandomForestClassifier(random_state=42)\n    \n    # Get cross-validated predictions\n    y_pred = cross_val_predict(rf_model, X, y, cv=cv)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y, y_pred)\n    precision = precision_score(y, y_pred)\n    recall = recall_score(y, y_pred)\n    f1 = f1_score(y, y_pred)\n\n    \"\"\"\n    print(\"\\nPerformance Metrics:\")\n    print(f\"Accuracy:  {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall:    {recall:.4f}\")\n    print(f\"F1 Score:  {f1:.4f}\")\n    \"\"\"\n\n    # Fit the final model on all data to get feature importances\n    rf_model.fit(X, y)\n    \n    # Store results\n    results[target] = {\n        'model': rf_model,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n    }\n    \n# Summary table of results\nsummary_df = pd.DataFrame({\n    'Target': [t for t in targets],\n    'Accuracy': [results[t]['accuracy'] for t in targets],\n    'Precision': [results[t]['precision'] for t in targets],\n    'Recall': [results[t]['recall'] for t in targets],\n    'F1 Score': [results[t]['f1'] for t in targets],\n})\n\n\nThe table given below provides performance measures for our machine learning models.\n\n\n\nTarget Variable\nAccuracy\nPrecision\nRecall\nF1 Score\n\n\n\n\nKnowledge Gain\n0.351\n0.375\n0.474\n0.419\n\n\nT2_Post_effectiveness\n0.558\n0.500\n0.382\n0.433\n\n\n\n\n\n\n\n\n\n Performance\n\n\n\n\n\nKnowledge Gain Model\n‚ùå Underperformed than the chance (Chance model performance .50)\nEffectiveness Model\n‚ö†Ô∏è Marginally better than chance (Accuracy = 0.558)\n\n\n\n\n\nPrediction using self-reported features\nThe augmentation of behavioral features with self-reported measures produced following results:\n\nKnowledge Gain Prediction\n‚ñ∂ Persistent sub-chance performance (Accuracy = 0.325) for predicting knowledge gain.\nPerceived Effectiveness Prediction\n‚ñ∂ Significant improvement (Accuracy = 0.792) demonstrates strong utility of subjective measures for evaluating self-assessed experiences\n\n\n\nCode\nextended_features = indp.copy()\nadditional_features = [col for col in df_sel.columns if col not in indp + targets ]\nextended_features.extend(additional_features)\n\n\n# Create a merged dataframe with all needed columns\nmerged_df = clean_df.copy()\nfor col in additional_features:\n    if col not in merged_df.columns:\n        merged_df[col] = df_sel[col]\n\nresults = {}\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor target in targets:\n    #print(f\"\\n{'='*70}\")\n    #print(f\"Building Random Forest classification model for target: {target}\")\n    #print(f\"{'='*70}\")\n    \n    # Create binary target using median split\n    median_value = target_split_point[target]\n    binary_target_name = f\"{target}_binary\"\n    merged_df[binary_target_name] = (merged_df[target] &gt; median_value).astype(int)\n    \n    ##print(f\"Created binary target '{binary_target_name}' with median split at {median_value}\")\n    #print(f\"Class distribution: {merged_df[binary_target_name].value_counts().to_dict()}\")\n    \n    # Prepare X and y\n    X = merged_df[extended_features]\n    y = merged_df[binary_target_name]\n    \n    # Create Random Forest classifier with default parameters\n    rf_model = RandomForestClassifier(random_state=42)\n    \n    # Get cross-validated predictions and probabilities\n    y_pred = cross_val_predict(rf_model, X, y, cv=cv)\n    y_prob = cross_val_predict(rf_model, X, y, cv=cv, method='predict_proba')[:, 1]\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y, y_pred)\n    precision = precision_score(y, y_pred)\n    recall = recall_score(y, y_pred)\n    f1 = f1_score(y, y_pred)\n\n    \"\"\"\n    print(\"\\nPerformance Metrics:\")\n    print(f\"Accuracy:  {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall:    {recall:.4f}\")\n    print(f\"F1 Score:  {f1:.4f}\")\n    \"\"\"\n    \n    # Fit the final model on all data to get feature importances\n    rf_model.fit(X, y)\n    \n    # Store results\n    results[target] = {\n        'model': rf_model,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n    }\n\n# Summary table of results\nsummary_df = pd.DataFrame({\n    'Target': [t for t in targets],\n    'Accuracy': [results[t]['accuracy'] for t in targets],\n    'Precision': [results[t]['precision'] for t in targets],\n    'Recall': [results[t]['recall'] for t in targets],\n    'F1 Score': [results[t]['f1'] for t in targets],\n})\n\n#print(\"\\nSummary of Model Performance:\")\n#print(summary_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Target\nAccuracy\nPrecision\nRecall\nF1 Score\n\n\n\n\nKnowledge Gain\n0.325\n0.306\n0.290\n0.297\n\n\nT2_Post_effectiveness\n0.792\n0.781\n0.735\n0.758\n\n\n\nFigure¬†1 shows importance of features for perceived effectiveness prediction.\n\n\nCode\nresutl = results['T2_Post_effectiveness']\n\n# Get the fitted Random Forest model\nrf_model = result['model']\n\n# Get and scale feature importances\nimportances = rf_model.feature_importances_\nimportance_percent = (importances / importances.max()) * 100  # Scale to percentage\n\n# Create and sort DataFrame\nfeature_importance = pd.DataFrame({\n    'Feature': extended_features,\n    'Importance': importances,\n    'Importance (%)': importance_percent\n}).sort_values('Importance (%)', ascending=False)\n\n# Plot settings\nplt.figure(figsize=(10, 6))\nax = sns.barplot(x='Importance (%)', \n                 y='Feature', \n                 data=feature_importance.head(top_n),\n                 palette='viridis',  # Color gradient\n                 edgecolor='black',  # Outline bars\n                 linewidth=0.5)\n\n# Add value labels\nfor i, (_, row) in enumerate(feature_importance.head(top_n).iterrows()):\n    ax.text(row['Importance (%)'] + 1,  # x-position\n            i,  # y-position\n            f\"{row['Importance (%)']:.1f}%\",  # Formatted text\n            va='center',\n            fontsize=10)\n\n# Styling\nplt.title(f'Top {top_n} Predictive Features for perceived learning effectiveness\\n(Relative Importance)', \n          fontsize=14, pad=20)\nplt.xlabel('Relative Importance (%)', fontsize=12)\nplt.ylabel('')  # Remove 'Feature' y-label for cleaner look\nsns.despine(left=True)  # Remove left spine\nplt.grid(axis='x', alpha=0.3)  # Add subtle grid\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: Feature importance for perceived learning effectiveness prediction model\n\n\n\n\n\n\n\n\n\n\n\nCaution with interpreting performance results\n\n\n\nThe goal was not here to build automated models for prediction purposes. Rather it was to investigate predictive nature of various features for a better understanding of relationships between behavioral & self-reported features and target variables."
  },
  {
    "objectID": "posts/pre.html",
    "href": "posts/pre.html",
    "title": "Cleaning and Preprocessing",
    "section": "",
    "text": "Several variables show missing entries (see Figure¬†1), particularly in episodes‚Äô task complexity. The task complexity in any episode is computed by taking an average of complexity levels of tasks completed by a student. Therefore, if a student has not completed any tasks in an episode then the task complexity for that episode is not defined.\n\n\nCode\nimport pandas as pd\n\n# load data\ndf = pd.read_csv('../Eduflex.xlsx - Pankajile.csv')\n\ndf.set_index('Student_ID',inplace=True)\n\n# columns containing knowledge-related data\nknowledge_cols = ['pre_knowl','post_knowl','knowledge gain']\n\n# columns containing self-reported data\nself_report_cols = ['T5_learning difficulty',\n                     'T2_Post_effectiveness',\n                     'T4_Post_needforhelp',\n                     'T7_shared_control_student',\n                     'T8_shared_teacher',\n                     'T11__cogn_load',\n                     'T13_own_effort',\n                     'T14_TAM_easy']\n\n# columns containing episodes data\nepisodes_cols =[item for item in df.columns.to_list() if item not in knowledge_cols + self_report_cols + ['Gender']]\n\n# knowledge data\ndf_knowledge = df[knowledge_cols]\n\n# self-reported data\ndf_self_report = df[self_report_cols]\n\n# episodes data\ndf_episodes = df[episodes_cols]\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate missing counts\nmissing_counts = df.isnull().sum().sort_values(ascending=False)\nmissing_counts = missing_counts[missing_counts &gt; 0]  # Only show variables with missing data\n\n# Create plot\nplt.figure(figsize=(8, 4))\nax = sns.barplot(x=missing_counts.index, \n                 y=missing_counts.values,\n                 palette=\"Reds_r\",  # Red gradient (dark = more missing)\n                 edgecolor='black',\n                 linewidth=0.5)\n\n# Add value labels\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', \n                xytext=(0, 5), \n                textcoords='offset points')\n\n# Style adjustments\nplt.title(\"\", pad=15)\nplt.xlabel(\"\")\nplt.ylabel(\"Number of Missing Values\")\nplt.xticks(rotation=90)\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: Missing Values\n\n\n\n\n\n\nHandling missing values\n\nWe will handle the missing values in the following manner.\n\nDelete all records where knowledge related information is missing (i.e., records having missing values for knowledge gain, pre knowledge or post knowledge)\nRemove records with missing values in T5_learning_difficulty.\nRemove episode E4 data. This episode consisting of two tasks and a significant number of students did not completed those tasks.\nImpute missing values for task complexity in rest of the episodes. This decision will allow us to generate new features (we will discuss them in the next section.\n\n\n\nCode\n# Delete all records where knowledge related information is missing\ndf_knowledge_clean = df_knowledge.dropna(axis=0, how='any')\n\n# Deleting records with missing value in T5_learning_difficulty column\ndf_self_report_clean = df_self_report.dropna(axis=0,how='any')\n\n# Removing E4 episode\ne4_non_cols = [item for item in df_episodes.columns if 'E4' not in item]\ndf_episodes_non_e4 = df_episodes[e4_non_cols]\n\n# Impute complexity with a very small number\ndf_episodes_clean = df_episodes_non_e4.fillna(0.00000001)\n\n# Final dataframe\nclean_df = df_knowledge_clean.join(df_self_report_clean, how='inner').join(df_episodes_clean, how='inner')\n\n\n\n\nCode\n# Set style\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(8, 3))\n\n# Create plot\nax = sns.barplot(x=[df.shape[0], clean_df.shape[0]], \n                 y=['Before handling missing', 'After handling missing'],\n                 palette=[\"#d62728\", \"#2ca02c\"],  # Red/Green color scheme\n                 saturation=0.8,\n                 width=0.6)\n\n# Add value labels\nfor i, v in enumerate([df.shape[0], clean_df.shape[0]]):\n    ax.text(v + max([df.shape[0], clean_df.shape[0]])*0.02, i, \n            f\"{v:,}\", \n            color='black', \n            va='center',\n            fontweight='bold')\n\n# Styling\nplt.title(\"Record Count Before vs After Handling Missing Values\", pad=15)\nplt.xlabel(\"Number of Records\", labelpad=10)\nplt.ylabel(\"\")\nsns.despine(left=True, bottom=True)\nax.grid(axis='x', linestyle='--', alpha=0.4)\nax.set_axisbelow(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†2: Before and after handling missing values\n\n\n\n\n\n\n\nRestructuring the data\n\nWe will now restructure the data for simplifying our analysis later. In this restructure, we will transform the Episode interaction data from Wide format to Long format. That is converting the data from its current form (each student‚Äôs record in a single row for all episodes) to a new form (one row will contain only interaction to a single episode).\nThe new data will look like the below dataframe.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# First, let's identify all episode prefixes (E1, E2, etc.)\nepisode_cols = [col for col in df.columns if col.startswith('E') and not '_' in col]\nepisode_prefixes = [col for col in episode_cols if col.replace('E','').isdigit()]\n\n# Create a list to store all transformed episode data\nepisode_data = []\n\nfor prefix in episode_prefixes:\n    # Get all columns for this episode\n    episode_cols = [col for col in df.columns if col.startswith(prefix + '_') or col == prefix]\n\n    \n    # Create a temporary DataFrame for this episode\n    episode_df = df[episode_cols].copy()\n\n    # Rename columns by removing the episode prefix\n    new_cols = {}\n    for col in episode_df.columns:\n        if col == prefix:\n            new_cols[col] = 'episode_id'\n        else:\n            new_cols[col] = col[len(prefix)+1:]  # Remove prefix and underscore\n    \n    episode_df = episode_df.rename(columns=new_cols)\n\n    episode_df['episode_id'] = prefix[1:]\n    \n    # Add student ID if available\n    episode_df['Student_ID'] = df.index\n    \n    \n    episode_data.append(episode_df)\n\n# Combine all episodes into one DataFrame\nlong_df = pd.concat(episode_data, ignore_index=True)\n\n# Clean up column names (handle inconsistent naming)\nlong_df.columns = long_df.columns.str.strip('_').str.lower()\n\n# Standardize column names (fix E13/E14 inconsistencies)\ncolumn_mapping = {\n    'total hints': 'total_hints',\n    'total materials': 'total_materials',\n    'total_activity': 'total_activity_count',\n    '__addmat_used': 'addmat_used'\n}\nlong_df = long_df.rename(columns=column_mapping)\n\n# Reorder columns if needed\nbase_cols = ['student_id', 'episode_id'] if 'student_id' in long_df.columns else ['episode_id']\nmetric_cols = [col for col in long_df.columns if col not in base_cols]\nlong_df = long_df[base_cols + sorted(metric_cols)]\n\n# Converting task_complexity data type\nlong_df.task_complexity = pd.to_numeric(long_df.task_complexity, errors='coerce')\nlong_df.fillna({'task_complexity':1}, inplace=True)\n\n\nlong_df.head()\n\n\n\n\n\n\n\n\n\nstudent_id\nepisode_id\naddmat_used\nhints_used\ntask_complexity\ntasks_completed\ntotal_activity_count\ntotal_hints\ntotal_materials\ntotal_tasks\n\n\n\n\n0\nkeila1\n1\n0.0\n0.00\n1.43\n87.5\n42\n3.0\n2.0\n8\n\n\n1\nkeila2\n1\n0.0\n66.67\n1.43\n87.5\n75\n3.0\n2.0\n8\n\n\n2\nkeila3\n1\n0.0\n0.00\n1.33\n37.5\n28\n3.0\n2.0\n8\n\n\n3\nkeila4\n1\n0.0\n66.67\n1.50\n50.0\n73\n3.0\n2.0\n8\n\n\n4\nkeila5\n1\n0.0\n33.33\n1.50\n50.0\n41\n3.0\n2.0\n8\n\n\n\n\n\n\n\n\nColumn explanation\n\n\n\n\n\n\n\n\nColumn Name\nDescription\nExample\n\n\n\n\nepisode_id\nUnique identifier for the learning episode (e.g., 1, 2)\n\"10\"\n\n\nstudent_id\nUnique identifier for the student\n\"keila1\"\n\n\naddmat_used\nPercentage of additional/supplementary materials accessed by the student\n50\n\n\nhints_used\nPercentage of hints requested by the student during tasks\n33.3\n\n\ntask_complexity\nNumeric measure of task difficulty of tasks completed by the student (higher = more complex)\n0.75\n\n\ntasks_completed\nPercentage of tasks successfully finished by the student\n50\n\n\ntotal_activity_count\nTotal interactions/actions taken by the student in the episode\n17\n\n\ntotal_hints\nMaximum hints available in the episode\n5\n\n\ntotal_materials\nTotal supplementary materials available in the episode\n4\n\n\ntotal_tasks\nTotal tasks available in the episode\n8\n\n\n\n\n\n\nNew feature generation\n\nNow, we will generate some new features from the existing ones. The goal is to take complexity into account to generate measures for hints used and task completed.\nLogic\n\nadjusted_hints_used = hints_used / task_complexity : If a student has used a same percentage of hints in two episodes having tasks with different complexity then hints used for difficult tasks will be treated lower than hints used for simple tasks.\nadjusted_task_completion = tasks_completed * task_complexity: If a student has completed a same percentage of tasks in two episodes having tasks with different complexity then completion for difficult tasks will be treated higher than task completion of simple tasks.\n\n\n\nCode\n# New feature generation\nlong_df['adjusted_hints_used'] = (long_df['hints_used'] * long_df['total_hints'] * .001) / long_df['task_complexity']\nlong_df['adjusted_tasks_completed'] = (long_df['tasks_completed'] * long_df['total_hints'] * .001) * long_df['task_complexity']\nlong_df['adjusted_activities'] = long_df['total_activity_count'] / long_df['total_tasks']\n# Save the DataFrame\nlong_df.to_csv('restructure_vara_data.csv',index=False)\n\n\n\n\nAggregating behavioral measures for all episodes\nWe will now aggregate episodes data for each students. We will obtain the following measures for three attributes: adjusted_hints_used, adjusted_tasks_completed, adjusted_activities\n\nMean\nMedian\nStandard deviation\nTotal\nQuartile-1\nQuartile-3\n\n\n\nCode\n# Group by student_id and calculate statistics for selected columns\nstudent_stats = long_df.groupby('student_id').agg({\n    'adjusted_hints_used': [\n        ('mean', 'mean'),\n        ('total', 'sum'),\n        ('std', 'std'),\n        ('median', 'median'),\n        ('q1', lambda x: x.quantile(0.25)),\n        ('q3', lambda x: x.quantile(0.75))\n    ],\n    'adjusted_tasks_completed': [\n        ('mean', 'mean'),\n        ('total', 'sum'),\n        ('std', 'std'),\n        ('median', 'median'),\n        ('q1', lambda x: x.quantile(0.25)),\n        ('q3', lambda x: x.quantile(0.75))\n    ],\n    'adjusted_activities': [\n        ('mean', 'mean'),\n        ('total', 'sum'),\n        ('std', 'std'),\n        ('median', 'median'),\n        ('q1', lambda x: x.quantile(0.25)),\n        ('q3', lambda x: x.quantile(0.75))\n    ]\n})\n\n# Flatten multi-level column names\nstudent_stats.columns = ['_'.join(col).strip() for col in student_stats.columns.values]\n\n# Reset index to keep student_id as a column\nstudent_stats = student_stats.reset_index()\n\n# Saving cleaned data\nstudent_stats.to_csv('agg_behavior_metrics.csv',index=False)\n\ndf_self_report_clean['student_id']= df_self_report_clean.index\ndf_self_report_clean.to_csv('self_report_clean.csv',index=False)\n\ndf_knowledge_clean['student_id']= df_knowledge_clean.index\ndf_knowledge_clean.to_csv('knowledge_clean.csv',index=False)\n\nstudent_stats.head()\n\n\n/var/folders/yz/d_7lkc5d3136jv69fndfsmfm0000gn/T/ipykernel_1398/2983732196.py:38: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_self_report_clean['student_id']= df_self_report_clean.index\n/var/folders/yz/d_7lkc5d3136jv69fndfsmfm0000gn/T/ipykernel_1398/2983732196.py:41: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_knowledge_clean['student_id']= df_knowledge_clean.index\n\n\n\n\n\n\n\n\n\nstudent_id\nadjusted_hints_used_mean\nadjusted_hints_used_total\nadjusted_hints_used_std\nadjusted_hints_used_median\nadjusted_hints_used_q1\nadjusted_hints_used_q3\nadjusted_tasks_completed_mean\nadjusted_tasks_completed_total\nadjusted_tasks_completed_std\nadjusted_tasks_completed_median\nadjusted_tasks_completed_q1\nadjusted_tasks_completed_q3\nadjusted_activities_mean\nadjusted_activities_total\nadjusted_activities_std\nadjusted_activities_median\nadjusted_activities_q1\nadjusted_activities_q3\n\n\n\n\n0\nkeila1\n0.024041\n0.288497\n0.060975\n0.0\n0.0\n0.000000\n0.113145\n1.357741\n0.180667\n0.000000\n0.0\n0.135000\n9.749196\n136.488745\n7.902623\n10.943182\n1.312500\n14.835714\n\n\n1\nkeila11\n0.008332\n0.099990\n0.028865\n0.0\n0.0\n0.000000\n0.209937\n2.519240\n0.300722\n0.095455\n0.0\n0.318844\n11.939590\n167.154257\n7.239382\n12.055556\n7.718750\n16.031818\n\n\n2\nkeila12\n0.008332\n0.099990\n0.028865\n0.0\n0.0\n0.000000\n0.087322\n1.047865\n0.128274\n0.010795\n0.0\n0.153881\n5.480523\n76.727327\n5.265849\n4.341667\n1.671875\n7.642857\n\n\n3\nkeila13\n0.025626\n0.307517\n0.046397\n0.0\n0.0\n0.024997\n0.182172\n2.186060\n0.234867\n0.095455\n0.0\n0.306375\n13.117858\n183.650018\n9.252567\n14.085227\n4.500000\n19.980952\n\n\n4\nkeila14\n0.028321\n0.339857\n0.052167\n0.0\n0.0\n0.024997\n0.284265\n3.411185\n0.476059\n0.095455\n0.0\n0.365250\n17.456810\n244.395346\n8.207705\n16.375000\n11.017857\n21.779167"
  },
  {
    "objectID": "posts/analysis.html",
    "href": "posts/analysis.html",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "Now, we will perform an exploratory analysis of preprocessed data to uncover links between behavioral, self-reported and knowledge data. In particular, we will investigate How student behavior during the trajectory (as a proxy for metacognitive engagement) relates to self-reported perceptions (e.g., effort, cognitive load, perceived difficulties), and whether these behaviors predict or moderate perceived effectiveness and knowledge gain.\nCode\nimport pandas as pd\n\n# Load episodes data\ndf_ep = pd.read_csv('agg_behavior_metrics.csv')\ndf_ep.set_index('student_id', inplace=True)\n\n# Load knowledge gain data\ndf_kno = pd.read_csv('knowledge_clean.csv')\ndf_kno.set_index('student_id', inplace=True)\n\n# Load self reported data\ndf_sel = pd.read_csv('self_report_clean.csv')\ndf_sel.set_index('student_id', inplace=True)\n\nclean_df = pd.concat([df_ep,df_kno,df_sel], join='inner', axis=1)"
  },
  {
    "objectID": "posts/analysis.html#removing-correlated-behavioral-features",
    "href": "posts/analysis.html#removing-correlated-behavioral-features",
    "title": "Exploratory Analysis",
    "section": "Removing correlated behavioral features",
    "text": "Removing correlated behavioral features\nFigure¬†1 displays correlation among behavioral features. We will remove one from each pair of behavioral features having correation &gt; .90.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Step 1: Calculate the correlation matrix\ncorrelation_matrix = df_ep.corr().abs()\n\n# Step 2: Create a mask for the upper triangle of the correlation matrix\nmask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n\n# Step 3: Set a correlation threshold (commonly 0.7 or 0.8)\nthreshold = 0.8\n\n# Step 4: Find features with correlation greater than the threshold\nto_drop = []\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i+1, len(correlation_matrix.columns)):\n        if correlation_matrix.iloc[i, j] &gt; threshold:\n            colname = correlation_matrix.columns[j]\n            to_drop.append(colname)\n            \n# Remove duplicate features to drop\nto_drop = list(set(to_drop))\n#print(f\"Features to drop: {\"\\n\\t\".join(to_drop)}\")\n\n# Step 5: Create a new DataFrame without the correlated features\ndf_ep_uncorrelated = df_ep.drop(columns=to_drop)\n\n# Visualize the correlation matrix before removing features with correlation values\nplt.figure(figsize=(14, 12))\nplt.title(\"Correlation among behavioral features\", fontsize=16)\n# Include the correlation values in the heatmap\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', mask=mask, \n            fmt='.2f', annot_kws={\"size\": 8}, linewidths=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: Behavioral correlated features\n\n\n\n\n\nOur final set of features for subsequent analysis is given in the following table.\n\n\n\n\n\n\n\n\n#\nFeature Name\nDescription (Inferred)\n\n\n\n\n1\nadjusted_hints_used_mean\nMean number of hints used (adjusted)\n\n\n2\nadjusted_hints_used_q1\nFirst quartile of hints used (adjusted)\n\n\n3\nadjusted_tasks_completed_mean\nMean number of tasks completed (adjusted)\n\n\n4\nadjusted_tasks_completed_q1\nFirst quartile of tasks completed (adjusted)\n\n\n5\nadjusted_activities_mean\nMean activity metric (adjusted)\n\n\n6\nadjusted_activities_std\nStandard deviation of activities (adjusted)"
  },
  {
    "objectID": "posts/analysis.html#compute-correlation-between-behavioral-features-and-knowledge-perception-measures",
    "href": "posts/analysis.html#compute-correlation-between-behavioral-features-and-knowledge-perception-measures",
    "title": "Exploratory Analysis",
    "section": "Compute correlation between behavioral features and knowledge & perception measures",
    "text": "Compute correlation between behavioral features and knowledge & perception measures\nWe will apply the Spearman method to compute correlation between selected behavioral features and knowledge & perception measures. Figure¬†2 shows correlation measures along with their statistical significance displayed using *.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\ndef compute_correlation_with_significance(df, group1, group2, alpha=0.05):\n    \"\"\"\n    Compute correlation between features in group1 and group2,\n    and determine statistical significance.\n    \n    Returns:\n    - corr_matrix: correlation matrix\n    - p_matrix: matrix of p-values\n    - significant_matrix: boolean matrix indicating significant correlations\n    \"\"\"\n    # Initialize matrices\n    corr_matrix = pd.DataFrame(index=group1, columns=group2)\n    p_matrix = pd.DataFrame(index=group1, columns=group2)\n    \n    # Calculate correlation and p-value for each pair\n    for feat1 in group1:\n        for feat2 in group2:\n            # Calculate Pearson correlation and p-value\n            corr, p_value = stats.pearsonr(df[feat1].dropna(), df[feat2].dropna())\n            corr_matrix.loc[feat1, feat2] = corr\n            p_matrix.loc[feat1, feat2] = p_value\n    \n    # Create a matrix indicating significant correlations\n    significant_matrix = p_matrix &lt; alpha\n    \n    return corr_matrix, p_matrix, significant_matrix\n\n\ngroup1 = ['adjusted_hints_used_mean', 'adjusted_tasks_completed_mean', 'adjusted_tasks_completed_q1',\n       'adjusted_activities_mean', 'adjusted_activities_std']\ngroup2 = df_kno.columns.to_list() + df_sel.columns.to_list()\n\n\n# Compute correlation and significance\ncorr_matrix, p_matrix, significant_matrix = compute_correlation_with_significance(\n    clean_df, group1, group2\n)\n\ncorr_matrix = corr_matrix.astype(float)\n\n# Create a custom annotation matrix\ndef create_annot_matrix(corr_matrix, significant_matrix):\n    \"\"\"Create annotation matrix with correlation values and stars for significance\"\"\"\n    annot_matrix = corr_matrix.applymap(lambda x: f\"{x:.2f}\")\n    \n    # Add stars for significant correlations\n    for i in range(len(corr_matrix.index)):\n        for j in range(len(corr_matrix.columns)):\n            if significant_matrix.iloc[i, j]:\n                annot_matrix.iloc[i, j] = annot_matrix.iloc[i, j] + \"*\"\n    \n    return annot_matrix\n\n# Create annotation matrix\nannot_matrix = create_annot_matrix(corr_matrix, significant_matrix)\n\n# Plot the correlation heatmap\nplt.figure(figsize=(max(8, len(group2)), max(6, len(group1))))\nsns.heatmap(\n    corr_matrix, \n    annot=annot_matrix, \n    fmt=\"\", \n    cmap=\"coolwarm\", \n    vmin=-1, \n    vmax=1,\n    linewidths=0.5,\n    cbar_kws={\"label\": \"Correlation Coefficient\"}\n)\nplt.title(\"Correlation Between Feature Groups (Spearman)\", fontsize=14)\nplt.xlabel(\"Behavioral Features\", fontsize=12)\nplt.ylabel(\"Knowledge & Perception Features\", fontsize=12)\nplt.tight_layout()\n\nplt.show()\n\n# Optional: Display the actual p-values\n#print(\"P-values for correlations:\")\n#print(p_matrix.round(4))\n\n\n\n\n\n\n\n\nFigure¬†2: Spearman correlation between behavioral features and knowledge & perception measures"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Impact analysis of Flexible Learning Trajectories",
    "section": "",
    "text": "This report documents a comprehensive analysis of data obtained from VARA platform in the context of flexible learning trajectories:\n\nBehavioral interactions & Self-reported measures\n\nStudents‚Äô interaction data\nSurveys responses\n\nLearning outcomes:\n\nKnowledge gain (objective assessment scores)\n\nPerceived effectiveness (subjective learning evaluations)\n\n\n\n\nKey Insights\n\nüîç Feature-Target Relationships\n\nSelf-reports strongly predict perceived effectiveness (Accuracy = 0.79) but poorly explain knowledge gain (Accuracy = 0.33)\n\nBehavioral patterns alone show limited predictive power for either outcome"
  }
]