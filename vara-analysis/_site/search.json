[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/data.html",
    "href": "posts/data.html",
    "title": "Learning Trajectories Dataset",
    "section": "",
    "text": "This dataset captures student interactions with VARA, a digital learning platform where educational materials are organized as structured learning trajectories. These trajectories consist of:\n\nSequential Episodes\n\nArranged in dependent order\n\nEach targets a specific concept or skill\n\nFlexible Activities\n\nStudent-directed exploration within episodes\n\nMultiple attempts allowed\n\nBuilt-in hints and supplementary materials\n\n\nStudents interactions to learning trajectories are recorded and processed to extract behavioral features such as number of attempts, time spent, completions, etc. Additionally, students’ metacognitive indicators are obtained through self-reported data.\n\nDataset samples\nThe dataset contains interaction records of 108 students with 14 episodes. The snapshots below provide samples of dataset.\n\n\nCode\nimport pandas as pd\n\n# load data\ndf = pd.read_csv('../Eduflex.xlsx - Pankajile.csv')\n\n# columns containing knowledge-related data\nknowledge_cols = ['pre_knowl','post_knowl','knowledge gain']\n\n# columns containing self-reported data\nself_report_cols = ['T5_learning difficulty',\n                     'T2_Post_effectiveness',\n                     'T4_Post_needforhelp',\n                     'T7_shared_control_student',\n                     'T8_shared_teacher',\n                     'T11__cogn_load',\n                     'T13_own_effort',\n                     'T14_TAM_easy']\n\n# columns containing episodes data\nepisodes_cols =[item for item in df.columns.to_list() if item not in knowledge_cols + self_report_cols + ['Gender']]\n\n# knowledge data\ndf_knowledge = df[knowledge_cols]\n\n# self-reported data\ndf_self_report = df[self_report_cols]\n\n# episodes data\ndf_episodes = df[episodes_cols]\n\ndf_knowledge.head()\n\n\n\n\n\n\n\n\n\npre_knowl\npost_knowl\nknowledge gain\n\n\n\n\n0\n0.49\n0.60\n0.11\n\n\n1\n0.86\n0.97\n0.11\n\n\n2\nNaN\nNaN\nNaN\n\n\n3\n0.93\n0.97\n0.04\n\n\n4\n0.88\n0.87\n-0.01\n\n\n\n\n\n\n\n\n\nCode\ndf.shape\n\n\n(108, 133)\n\n\n\n\nCode\ndf_self_report.head()\n\n\n\n\n\n\n\n\n\nT5_learning difficulty\nT2_Post_effectiveness\nT4_Post_needforhelp\nT7_shared_control_student\nT8_shared_teacher\nT11__cogn_load\nT13_own_effort\nT14_TAM_easy\n\n\n\n\n0\nNaN\n4.67\n4.5\n5.00\n3.00\n1.00\n5.00\n3.8\n\n\n1\n3.4\n1.67\n3.0\n5.00\n5.00\n5.00\n5.00\n4.2\n\n\n2\n3.6\n3.00\n3.0\n3.25\n3.00\n3.00\n3.00\n3.6\n\n\n3\n2.8\n3.00\n2.0\n3.25\n4.75\n2.63\n3.75\n3.8\n\n\n4\n2.6\n2.67\n3.0\n2.75\n3.75\n2.88\n2.75\n4.6\n\n\n\n\n\n\n\n\n\nCode\ndf_episodes.head()\n\n\n\n\n\n\n\n\n\nStudent_ID\nE1\nE1_total_tasks\nE1_tasks_completed\nE1_task_complexity\nE1_total_hints\nE1_hints_used\nE1_total_materials\nE1_addMat_used\nE1total_activity_count\n...\nE13_total_activity_count\nE14\nE14_total_tasks\nE14_tasks_completed\nE14_task_complexity\nE14_total hints\nE14_total_hints_used\nE14_total materials\nE14__addMat_used\nE14_total_activity\n\n\n\n\n0\nkeila1\n0.22\n8\n87.5\n1.43\n3\n0.00\n2\n0\n42\n...\n46\n284\n8\n62.5\n1.8\n0\n0\n4\n0\n82\n\n\n1\nkeila2\n0.79\n8\n87.5\n1.43\n3\n66.67\n2\n0\n75\n...\n46\n284\n8\n62.5\n1.8\n0\n0\n4\n0\n79\n\n\n2\nkeila3\n75.00\n8\n37.5\n1.33\n3\n0.00\n2\n0\n28\n...\n62\n284\n8\n12.5\n1.0\n0\n0\n4\n0\n39\n\n\n3\nkeila4\n75.00\n8\n50.0\n1.50\n3\n66.67\n2\n0\n73\n...\n55\n284\n8\n62.5\n1.8\n0\n0\n4\n0\n138\n\n\n4\nkeila5\n75.00\n8\n50.0\n1.50\n3\n33.33\n2\n0\n41\n...\n94\n284\n8\n62.5\n1.8\n0\n0\n4\n0\n66\n\n\n\n\n5 rows × 121 columns\n\n\n\n\n\n\n\n\n\nMissing values\n\n\n\nSome of the data columns in the datasets contain missing values which need to be handled before moving to analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Analytics Research Group",
    "section": "",
    "text": "Welcome to Our Research Group\nWe investigate how data and technology can transform education at Tallinn University, bridging: - Learning sciences with computational analytics\n- Pedagogical theories with AI-driven tools\n- Classroom practices with scalable innovations\nOur work focuses on ethical, evidence-based solutions for modern education challenges.\n\n\n\n\n\nConnecting the dots"
  },
  {
    "objectID": "posts/data.html#dataset-samples",
    "href": "posts/data.html#dataset-samples",
    "title": "Learning Trajectories Dataset",
    "section": "Dataset samples",
    "text": "Dataset samples\n\n\nCode\nimport pandas as pd\n\n# load data\ndf = pd.read_csv('../Eduflex.xlsx - Pankajile.csv')\n\n# columns containing knowledge-related data\nknowledge_cols = ['pre_knowl','post_knowl','knowledge gain']\n\n# columns containing self-reported data\nself_report_cols = ['T5_learning difficulty',\n                     'T2_Post_effectiveness',\n                     'T4_Post_needforhelp',\n                     'T7_shared_control_student',\n                     'T8_shared_teacher',\n                     'T11__cogn_load',\n                     'T13_own_effort',\n                     'T14_TAM_easy']\n\n# columns containing episodes data\nepisodes_cols =[item for item in df.columns.to_list() if item not in knowledge_cols + self_report_cols + ['Gender']]\n\n# knowledge data\ndf_knowledge = df[knowledge_cols]\n\n# self-reported data\ndf_self_report = df[self_report_cols]\n\n# episodes data\ndf_episodes = df[episodes_cols]\n\n\n\n\nCode\ndf_knowledge.head()\n\n\n\n\n\n\n\n\n\npre_knowl\npost_knowl\nknowledge gain\n\n\n\n\n0\n0.49\n0.60\n0.11\n\n\n1\n0.86\n0.97\n0.11\n\n\n2\nNaN\nNaN\nNaN\n\n\n3\n0.93\n0.97\n0.04\n\n\n4\n0.88\n0.87\n-0.01\n\n\n\n\n\n\n\n\n\nCode\ndf_self_report.head()\n\n\n\n\n\n\n\n\n\nT5_learning difficulty\nT2_Post_effectiveness\nT4_Post_needforhelp\nT7_shared_control_student\nT8_shared_teacher\nT11__cogn_load\nT13_own_effort\nT14_TAM_easy\n\n\n\n\n0\nNaN\n4.67\n4.5\n5.00\n3.00\n1.00\n5.00\n3.8\n\n\n1\n3.4\n1.67\n3.0\n5.00\n5.00\n5.00\n5.00\n4.2\n\n\n2\n3.6\n3.00\n3.0\n3.25\n3.00\n3.00\n3.00\n3.6\n\n\n3\n2.8\n3.00\n2.0\n3.25\n4.75\n2.63\n3.75\n3.8\n\n\n4\n2.6\n2.67\n3.0\n2.75\n3.75\n2.88\n2.75\n4.6\n\n\n\n\n\n\n\n\n\nCode\ndf_episodes.head()\n\n\n\n\n\n\n\n\n\nStudent_ID\nE1\nE1_total_tasks\nE1_tasks_completed\nE1_task_complexity\nE1_total_hints\nE1_hints_used\nE1_total_materials\nE1_addMat_used\nE1total_activity_count\n...\nE13_total_activity_count\nE14\nE14_total_tasks\nE14_tasks_completed\nE14_task_complexity\nE14_total hints\nE14_total_hints_used\nE14_total materials\nE14__addMat_used\nE14_total_activity\n\n\n\n\n0\nkeila1\n0.22\n8\n87.5\n1.43\n3\n0.00\n2\n0\n42\n...\n46\n284\n8\n62.5\n1.8\n0\n0\n4\n0\n82\n\n\n1\nkeila2\n0.79\n8\n87.5\n1.43\n3\n66.67\n2\n0\n75\n...\n46\n284\n8\n62.5\n1.8\n0\n0\n4\n0\n79\n\n\n2\nkeila3\n75.00\n8\n37.5\n1.33\n3\n0.00\n2\n0\n28\n...\n62\n284\n8\n12.5\n1.0\n0\n0\n4\n0\n39\n\n\n3\nkeila4\n75.00\n8\n50.0\n1.50\n3\n66.67\n2\n0\n73\n...\n55\n284\n8\n62.5\n1.8\n0\n0\n4\n0\n138\n\n\n4\nkeila5\n75.00\n8\n50.0\n1.50\n3\n33.33\n2\n0\n41\n...\n94\n284\n8\n62.5\n1.8\n0\n0\n4\n0\n66\n\n\n\n\n5 rows × 121 columns"
  },
  {
    "objectID": "posts/data.html#missing-values",
    "href": "posts/data.html#missing-values",
    "title": "Learning Trajectories Dataset",
    "section": "Missing values",
    "text": "Missing values\nSome of the data columns in the datasets contain missing values which need to be handled before moving to analysis."
  },
  {
    "objectID": "posts/pre.html",
    "href": "posts/pre.html",
    "title": "Cleaning and Preprocessing",
    "section": "",
    "text": "Several variables show missing entries (see Figure 1), particularly in episodes’ task complexity. The task complexity in any episode is computed by taking an average of complexity levels of tasks completed by a student. Therefore, if a student has not completed any tasks in an episode then the task complexity for that episode is not defined.\n\n\nCode\nimport pandas as pd\n\n# load data\ndf = pd.read_csv('../Eduflex.xlsx - Pankajile.csv')\n\ndf.set_index('Student_ID',inplace=True)\n\n# columns containing knowledge-related data\nknowledge_cols = ['pre_knowl','post_knowl','knowledge gain']\n\n# columns containing self-reported data\nself_report_cols = ['T5_learning difficulty',\n                     'T2_Post_effectiveness',\n                     'T4_Post_needforhelp',\n                     'T7_shared_control_student',\n                     'T8_shared_teacher',\n                     'T11__cogn_load',\n                     'T13_own_effort',\n                     'T14_TAM_easy']\n\n# columns containing episodes data\nepisodes_cols =[item for item in df.columns.to_list() if item not in knowledge_cols + self_report_cols + ['Gender']]\n\n# knowledge data\ndf_knowledge = df[knowledge_cols]\n\n# self-reported data\ndf_self_report = df[self_report_cols]\n\n# episodes data\ndf_episodes = df[episodes_cols]\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate missing counts\nmissing_counts = df.isnull().sum().sort_values(ascending=False)\nmissing_counts = missing_counts[missing_counts &gt; 0]  # Only show variables with missing data\n\n# Create plot\nplt.figure(figsize=(8, 4))\nax = sns.barplot(x=missing_counts.index, \n                 y=missing_counts.values,\n                 palette=\"Reds_r\",  # Red gradient (dark = more missing)\n                 edgecolor='black',\n                 linewidth=0.5)\n\n# Add value labels\nfor p in ax.patches:\n    ax.annotate(f\"{int(p.get_height())}\", \n                (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', \n                xytext=(0, 5), \n                textcoords='offset points')\n\n# Style adjustments\nplt.title(\"\", pad=15)\nplt.xlabel(\"\")\nplt.ylabel(\"Number of Missing Values\")\nplt.xticks(rotation=90)\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Missing Values\n\n\n\n\n\n\nHandling missing values\n\nWe will handle the missing values in the following manner.\n\nDelete all records where knowledge related information is missing (i.e., records having missing values for knowledge gain, pre knowledge or post knowledge)\nRemove records with missing values in T5_learning_difficulty.\nRemove episode E4 data. This episode consisting of two tasks and a significant number of students did not completed those tasks.\nImpute missing values for task complexity in rest of the episodes. This decision will allow us to generate new features (we will discuss them in the next section.\n\n\n\nCode\n# Delete all records where knowledge related information is missing\ndf_knowledge_clean = df_knowledge.dropna(axis=0, how='any')\n\n# Deleting records with missing value in T5_learning_difficulty column\ndf_self_report_clean = df_self_report.dropna(axis=0,how='any')\n\n# Removing E4 episode\ne4_non_cols = [item for item in df_episodes.columns if 'E4' not in item]\ndf_episodes_non_e4 = df_episodes[e4_non_cols]\n\n# Impute complexity with a very small number\ndf_episodes_clean = df_episodes_non_e4.fillna(0.00000001)\n\n# Final dataframe\nclean_df = df_knowledge_clean.join(df_self_report_clean, how='inner').join(df_episodes_clean, how='inner')\n\n\n\n\nCode\n# Set style\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(8, 3))\n\n# Create plot\nax = sns.barplot(x=[df.shape[0], clean_df.shape[0]], \n                 y=['Before handling missing', 'After handling missing'],\n                 palette=[\"#d62728\", \"#2ca02c\"],  # Red/Green color scheme\n                 saturation=0.8,\n                 width=0.6)\n\n# Add value labels\nfor i, v in enumerate([df.shape[0], clean_df.shape[0]]):\n    ax.text(v + max([df.shape[0], clean_df.shape[0]])*0.02, i, \n            f\"{v:,}\", \n            color='black', \n            va='center',\n            fontweight='bold')\n\n# Styling\nplt.title(\"Record Count Before vs After Handling Missing Values\", pad=15)\nplt.xlabel(\"Number of Records\", labelpad=10)\nplt.ylabel(\"\")\nsns.despine(left=True, bottom=True)\nax.grid(axis='x', linestyle='--', alpha=0.4)\nax.set_axisbelow(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Before and after handling missing values\n\n\n\n\n\nE1, E2 , E6 Prior knowledge\nE3, E4, E7, E9, E10, E12, E13   Getting familiar with new topic\nE5, E8, E11, E14    Practicing\n\n\nRestructuring the data\n\nWe will now restructure the data for simplifying our analysis later. In this restructure, we will transform the Episode interaction data from Wide format to Long format. That is converting the data from its current form (each student’s record in a single row for all episodes) to a new form (one row will contain only interaction to a single episode).\nThe new data will look like the below dataframe.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# First, let's identify all episode prefixes (E1, E2, etc.)\nepisode_cols = [col for col in df.columns if col.startswith('E') and not '_' in col]\nepisode_prefixes = [col for col in episode_cols if col.replace('E','').isdigit()]\n\n# Create a list to store all transformed episode data\nepisode_data = []\n\nfor prefix in episode_prefixes:\n    # Get all columns for this episode\n    episode_cols = [col for col in df.columns if col.startswith(prefix + '_') or col == prefix]\n\n    \n    # Create a temporary DataFrame for this episode\n    episode_df = df[episode_cols].copy()\n\n    # Rename columns by removing the episode prefix\n    new_cols = {}\n    for col in episode_df.columns:\n        if col == prefix:\n            new_cols[col] = 'episode_id'\n        else:\n            new_cols[col] = col[len(prefix)+1:]  # Remove prefix and underscore\n    \n    episode_df = episode_df.rename(columns=new_cols)\n\n    episode_df['episode_id'] = prefix[1:]\n    \n    # Add student ID if available\n    episode_df['Student_ID'] = df.index\n    \n    \n    episode_data.append(episode_df)\n\n# Combine all episodes into one DataFrame\nlong_df = pd.concat(episode_data, ignore_index=True)\n\n# Clean up column names (handle inconsistent naming)\nlong_df.columns = long_df.columns.str.strip('_').str.lower()\n\n# Standardize column names (fix E13/E14 inconsistencies)\ncolumn_mapping = {\n    'total hints': 'total_hints',\n    'total materials': 'total_materials',\n    'total_activity': 'total_activity_count',\n    '__addmat_used': 'addmat_used'\n}\nlong_df = long_df.rename(columns=column_mapping)\n\n# Reorder columns if needed\nbase_cols = ['student_id', 'episode_id'] if 'student_id' in long_df.columns else ['episode_id']\nmetric_cols = [col for col in long_df.columns if col not in base_cols]\nlong_df = long_df[base_cols + sorted(metric_cols)]\n\n# Converting task_complexity data type\nlong_df.task_complexity = pd.to_numeric(long_df.task_complexity, errors='coerce')\nlong_df.fillna({'task_complexity':0.000001}, inplace=True)\n\n\nlong_df.head()\n\n\n\n\n\n\n\n\n\nstudent_id\nepisode_id\naddmat_used\nhints_used\ntask_complexity\ntasks_completed\ntotal_activity_count\ntotal_hints\ntotal_materials\ntotal_tasks\n\n\n\n\n0\nkeila1\n1\n0.0\n0.00\n1.43\n87.5\n42\n3.0\n2.0\n8\n\n\n1\nkeila2\n1\n0.0\n66.67\n1.43\n87.5\n75\n3.0\n2.0\n8\n\n\n2\nkeila3\n1\n0.0\n0.00\n1.33\n37.5\n28\n3.0\n2.0\n8\n\n\n3\nkeila4\n1\n0.0\n66.67\n1.50\n50.0\n73\n3.0\n2.0\n8\n\n\n4\nkeila5\n1\n0.0\n33.33\n1.50\n50.0\n41\n3.0\n2.0\n8\n\n\n\n\n\n\n\n\nColumn explanation\n\n\n\n\n\n\n\n\nColumn Name\nDescription\nExample\n\n\n\n\nepisode_id\nUnique identifier for the learning episode (e.g., 1, 2)\n\"10\"\n\n\nstudent_id\nUnique identifier for the student\n\"keila1\"\n\n\naddmat_used\nPercentage of additional/supplementary materials accessed by the student\n50\n\n\nhints_used\nPercentage of hints requested by the student during tasks\n33.3\n\n\ntask_complexity\nNumeric measure of task difficulty of tasks completed by the student (higher = more complex)\n0.75\n\n\ntasks_completed\nPercentage of tasks successfully finished by the student\n50\n\n\ntotal_activity_count\nTotal interactions/actions taken by the student in the episode\n17\n\n\ntotal_hints\nMaximum hints available in the episode\n5\n\n\ntotal_materials\nTotal supplementary materials available in the episode\n4\n\n\ntotal_tasks\nTotal tasks available in the episode\n8\n\n\n\n\n\n\nNew feature generation\n\nNow, we will generate some new features from the existing ones. The goal is to take complexity into account to generate measures for hints used and task completed.\nLogic\n\nadjusted_hints_used = hints_used / task_complexity : If a student has used a same percentage of hints in two episodes having tasks with different complexity then hints used for difficult tasks will be treated lower than hints used for simple tasks.\nadjusted_task_completion = tasks_completed * task_complexity: If a student has completed a same percentage of tasks in two episodes having tasks with different complexity then completion for difficult tasks will be treated higher than task completion of simple tasks.\n\n\n\nCode\n# New feature generation\nlong_df['adjusted_hints_used'] = (long_df['hints_used'] * long_df['total_hints'] * .001) / long_df['task_complexity']\nlong_df['adjusted_tasks_completed'] = (long_df['tasks_completed'] * long_df['total_hints'] * .001) * long_df['task_complexity']\nlong_df['adjusted_activities'] = long_df['total_activity_count'] / long_df['total_tasks']\n# Save the DataFrame\nlong_df.to_csv('restructure_vara_data.csv',index=False)\n\n\n\n\nCode\n\n\n\n0       0.000000\n1       0.139867\n2       0.000000\n3       0.133340\n4       0.066660\n          ...   \n1507    0.000000\n1508    0.000000\n1509    0.000000\n1510    0.000000\n1511    0.000000\nName: adjusted_hints_used, Length: 1512, dtype: float64\n\n\n\n\nAggregating behavioral measures for all episodes\nWe will now aggregate episodes data for each students. We will obtain the following measures for three attributes: adjusted_hints_used, adjusted_tasks_completed, adjusted_activities\n\nMean\nMedian\nMin\nMax\nStandard deviation\nTotal\nQuartile-1\nQuartile-3\n\n\n\nCode\n# Group by student_id and calculate statistics for selected columns\nstudent_stats = long_df.groupby('student_id').agg({\n    'adjusted_hints_used': [\n        ('mean', 'mean'),\n        ('total', 'sum'),\n        ('std', 'std'),\n        ('median', 'median'),\n        ('q1', lambda x: x.quantile(0.25)),\n        ('q3', lambda x: x.quantile(0.75))\n    ],\n    'adjusted_tasks_completed': [\n        ('mean', 'mean'),\n        ('total', 'sum'),\n        ('std', 'std'),\n        ('median', 'median'),\n        ('q1', lambda x: x.quantile(0.25)),\n        ('q3', lambda x: x.quantile(0.75))\n    ],\n    'adjusted_activities': [\n        ('mean', 'mean'),\n        ('total', 'sum'),\n        ('std', 'std'),\n        ('median', 'median'),\n        ('q1', lambda x: x.quantile(0.25)),\n        ('q3', lambda x: x.quantile(0.75))\n    ]\n})\n\n# Flatten multi-level column names\nstudent_stats.columns = ['_'.join(col).strip() for col in student_stats.columns.values]\n\n# Reset index to keep student_id as a column\nstudent_stats = student_stats.reset_index()\n\n# Saving cleaned data\nstudent_stats.to_csv('agg_behavior_metrics.csv',index=False)\n\ndf_self_report_clean['student_id']= df_self_report_clean.index\ndf_self_report_clean.to_csv('self_report_clean.csv',index=False)\n\ndf_knowledge_clean['student_id']= df_knowledge_clean.index\ndf_knowledge_clean.to_csv('knowledge_clean.csv',index=False)\n\nstudent_stats.head()\n\n\n\n\n\n\n\n\n\nstudent_id\nadjusted_hints_used_mean\nadjusted_hints_used_total\nadjusted_hints_used_std\nadjusted_hints_used_median\nadjusted_hints_used_q1\nadjusted_hints_used_q3\nadjusted_tasks_completed_mean\nadjusted_tasks_completed_total\nadjusted_tasks_completed_std\nadjusted_tasks_completed_median\nadjusted_tasks_completed_q1\nadjusted_tasks_completed_q3\nadjusted_activities_mean\nadjusted_activities_total\nadjusted_activities_std\nadjusted_activities_median\nadjusted_activities_q1\nadjusted_activities_q3\n\n\n\n\n0\nkeila1\n0.024041\n0.288497\n0.060975\n0.0\n0.0\n0.000000\n0.113145\n1.357741\n0.180667\n0.000000\n0.0\n0.135000\n9.749196\n136.488745\n7.902623\n10.943182\n1.312500\n14.835714\n\n\n1\nkeila11\n0.008332\n0.099990\n0.028865\n0.0\n0.0\n0.000000\n0.209937\n2.519240\n0.300722\n0.095455\n0.0\n0.318844\n11.939590\n167.154257\n7.239382\n12.055556\n7.718750\n16.031818\n\n\n2\nkeila12\n0.008332\n0.099990\n0.028865\n0.0\n0.0\n0.000000\n0.087322\n1.047865\n0.128274\n0.010795\n0.0\n0.153881\n5.480523\n76.727327\n5.265849\n4.341667\n1.671875\n7.642857\n\n\n3\nkeila13\n0.025626\n0.307517\n0.046397\n0.0\n0.0\n0.024997\n0.122172\n1.466061\n0.167201\n0.045455\n0.0\n0.187219\n13.117858\n183.650018\n9.252567\n14.085227\n4.500000\n19.980952\n\n\n4\nkeila14\n0.028321\n0.339857\n0.052167\n0.0\n0.0\n0.024997\n0.284265\n3.411185\n0.476059\n0.095455\n0.0\n0.365250\n17.456810\n244.395346\n8.207705\n16.375000\n11.017857\n21.779167"
  },
  {
    "objectID": "posts/analysis.html",
    "href": "posts/analysis.html",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "Now, we will perform an exploratory analysis of preprocessed data to uncover links between behavioral, self-reported and knowledge data. In particular, we will investigate How student behavior during the trajectory (as a proxy for metacognitive engagement) relates to self-reported perceptions (e.g., effort, cognitive load, perceived difficulties), and whether these behaviors predict or moderate perceived effectiveness and knowledge gain.\nCode\nimport pandas as pd\n\n# Load episodes data\ndf_ep = pd.read_csv('agg_behavior_metrics.csv')\ndf_ep.set_index('student_id', inplace=True)\n\n# Load knowledge gain data\ndf_kno = pd.read_csv('knowledge_clean.csv')\ndf_kno.set_index('student_id', inplace=True)\n\n# Load self reported data\ndf_sel = pd.read_csv('self_report_clean.csv')\ndf_sel.set_index('student_id', inplace=True)\n\nclean_df = pd.concat([df_ep,df_kno,df_sel], join='inner', axis=1)"
  },
  {
    "objectID": "posts/analysis.html#top-correlations",
    "href": "posts/analysis.html#top-correlations",
    "title": "Exploratory Analysis",
    "section": "Top Correlations",
    "text": "Top Correlations\n\nPositive Correlations\n\n\n\n\n\n\n\n\n\nRank\nGroup1 Feature\nGroup2 Feature\nCorrelation\n\n\n\n\n1\nadjusted_activities_q3\nT14_TAM_easy\n0.35\n\n\n2\nadjusted_activities_q3\npost_knowl\n0.35\n\n\n3\nadjusted_hints_used_q3\nT4_Post_needforhelp\n0.33\n\n\n4\nadjusted_activities_std\nT5_learning difficulty\n0.31\n\n\n5\nadjusted_activities_median\nT14_TAM_easy\n0.31\n\n\n\n\n\nNegative Correlations\n\n\n\n\n\n\n\n\n\nRank\nGroup1 Feature\nGroup2 Feature\nCorrelation\n\n\n\n\n1\nadjusted_hints_used_q3\nT2_Post_effectiveness\n-0.19\n\n\n2\nadjusted_hints_used_std\nT2_Post_effectiveness\n-0.18\n\n\n3\nadjusted_hints_used_q3\npre_knowl\n-0.17\n\n\n4\nadjusted_hints_used_std\nT7_shared_control_student\n-0.17\n\n\n5\nadjusted_activities_q3\nT4_Post_needforhelp\n-0.16\n\n\n\n\n\nCode\ntop_neg\n\n\n\n\n\n\n\n\n\nGroup1\nGroup2\nCorrelation\n\n\n\n\n48\nadjusted_hints_used_q3\nT2_Post_effectiveness\n-0.189838\n\n\n26\nadjusted_hints_used_std\nT2_Post_effectiveness\n-0.181353\n\n\n44\nadjusted_hints_used_q3\npre_knowl\n-0.165779\n\n\n28\nadjusted_hints_used_std\nT7_shared_control_student\n-0.165341\n\n\n181\nadjusted_activities_q3\nT4_Post_needforhelp\n-0.164228"
  }
]